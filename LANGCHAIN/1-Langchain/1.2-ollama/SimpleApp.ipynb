{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Gen AI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BASE_MODEL_REPO_ID'] = \"llama3.2:3b\"\n",
    "os.environ['EMBEDDING_MODEL_REPO_ID'] = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_llm = ChatOllama(model=os.environ[\"BASE_MODEL_REPO_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--from the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x20d035861b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/evaluation\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate an agentHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorConceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationOn this pageEvaluation quick start\\nThis quick start will get you up and running with our evaluation SDK and Experiments UI.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openai pydanticyarn add langsmith openai zod\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=\"<your-langchain-api-key>\"# The example uses OpenAI, but it\\'s not necessary in generalexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n3. Import dependencies\\u200b\\nPythonTypeScriptfrom langsmith import wrappers, Clientfrom pydantic import BaseModel, Fieldfrom openai import OpenAIclient = Client()openai_client = wrappers.wrap_openai(OpenAI())import { Client } from \"langsmith\";import OpenAI from \"openai\";import { z } from \"zod\";import { zodResponseFormat } from \"openai/helpers/zod\";import type { EvaluationResult } from \"langsmith/evaluation\";import { evaluate } from \"langsmith/evaluation\";   const client = new Client();const openai = new OpenAI();\\n4. Create a dataset\\u200b\\nPythonTypeScript# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application# Create inputs and reference outputsexamples = [  (      \"Which country is Mount Kilimanjaro located in?\",      \"Mount Kilimanjaro is located in Tanzania.\",  ),  (      \"What is Earth\\'s lowest point?\",      \"Earth\\'s lowest point is The Dead Sea.\",  ),]inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]outputs = [{\"answer\": output_answer} for _, output_answer in examples]# Programmatically create a dataset in LangSmithdataset = client.create_dataset(  dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application  // Create inputs and reference outputsconst examples: [string, string][] = [[  \"Which country is Mount Kilimanjaro located in?\",  \"Mount Kilimanjaro is located in Tanzania.\",],[  \"What is Earth\\'s lowest point?\",  \"Earth\\'s lowest point is The Dead Sea.\",],];const inputs = examples.map(([inputPrompt]) => ({question: inputPrompt,}));const outputs = examples.map(([, outputAnswer]) => ({answer: outputAnswer,}));// Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait client.createExamples({inputs,outputs,datasetId: dataset.id,});\\n5. Define what you\\'re evaluating\\u200b\\nPythonTypeScript# Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:  response = openai_client.chat.completions.create(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },          { \"role\": \"user\", \"content\": inputs[\"question\"] },      ],  )  return { \"response\": response.choices[0].message.content.strip() }// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: string): Promise<{ response: string }> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response: response.choices[0].message.content?.trim() || \"\" };}\\n6. Define evaluator\\u200b\\nPythonTypeScript# Define instructions for the LLM judge evaluatorinstructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.\"\"\"# Define output schema for the LLM judgeclass Grade(BaseModel):  score: bool = Field(      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"  )# Define LLM judge that grades the accuracy of the response relative to reference outputdef accuracy(outputs: dict, reference_outputs: dict) -> bool:  response = openai_client.beta.chat.completions.parse(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer: {outputs[\"response\"]}\"\"\"          },      ],      response_format=Grade,  )  return response.choices[0].message.parsed.score// Define instructions for the LLM judge evaluatorconst instructions = `Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.`;// Define context for the LLM judge evaluatorconst context = `Ground Truth answer: {reference}; Student\\'s Answer: {prediction}`;// Define output schema for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM judge that grades the accuracy of the response relative to reference outputasync function accuracy({outputs,referenceOutputs,}: {outputs?: Record<string, string>;referenceOutputs?: Record<string, string>;}): Promise<EvaluationResult> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: instructions },    { role: \"user\", content: context.replace(\"{prediction}\", outputs?.answer || \"\").replace(\"{reference}\", referenceOutputs?.answer || \"\") }  ],  response_format: zodResponseFormat(ResponseSchema, \"response\")});return {  key: \"accuracy\",  score: ResponseSchema.parse(JSON.parse(response.choices[0].message.content || \"\")).score,};}\\n7. Run and view results\\u200b\\nPythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(  target,  data=\"Sample dataset\",  evaluators=[      accuracy,      # can add multiple evaluators here  ],  experiment_prefix=\"first-eval-in-langsmith\",  max_concurrency=2,)// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate((exampleInput) => {  return target(exampleInput.question);},{  data: \"Sample dataset\",  evaluators: [    accuracy,    // can add multiple evaluators here  ],  experimentPrefix: \"first-eval-in-langsmith\",  maxConcurrency: 2,});\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.\\n\\nNext steps\\u200b\\nFor conceptual explanations see the Conceptual guide.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment3. Import dependencies4. Create a dataset5. Define what you\\'re evaluating6. Define evaluator7. Run and view resultsNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data--> Docs --> Divide our text into chunks --> vectors --> Vector Embedding --> Vectore store db\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter =RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate an agentHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorConceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationOn this pageEvaluation quick start'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='This quick start will get you up and running with our evaluation SDK and Experiments UI.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openai pydanticyarn add langsmith openai zod\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=\"<your-langchain-api-key>\"# The example uses OpenAI, but it\\'s not necessary in generalexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n3. Import dependencies\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='3. Import dependencies\\u200b\\nPythonTypeScriptfrom langsmith import wrappers, Clientfrom pydantic import BaseModel, Fieldfrom openai import OpenAIclient = Client()openai_client = wrappers.wrap_openai(OpenAI())import { Client } from \"langsmith\";import OpenAI from \"openai\";import { z } from \"zod\";import { zodResponseFormat } from \"openai/helpers/zod\";import type { EvaluationResult } from \"langsmith/evaluation\";import { evaluate } from \"langsmith/evaluation\";   const client = new Client();const openai = new OpenAI();\\n4. Create a dataset\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application# Create inputs and reference outputsexamples = [  (      \"Which country is Mount Kilimanjaro located in?\",      \"Mount Kilimanjaro is located in Tanzania.\",  ),  (      \"What is Earth\\'s lowest point?\",      \"Earth\\'s lowest point is The Dead Sea.\",  ),]inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]outputs = [{\"answer\": output_answer} for _, output_answer in examples]# Programmatically create a dataset in LangSmithdataset = client.create_dataset(  dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application  // Create inputs and reference outputsconst examples: [string, string][] = [[  \"Which country is Mount Kilimanjaro located in?\",  \"Mount Kilimanjaro is located in Tanzania.\",],[  \"What is Earth\\'s lowest point?\",  \"Earth\\'s lowest point is The Dead Sea.\",],];const inputs = examples.map(([inputPrompt]) => ({question: inputPrompt,}));const outputs = examples.map(([, outputAnswer]) => ({answer: outputAnswer,}));// Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait client.createExamples({inputs,outputs,datasetId: dataset.id,});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"5. Define what you're evaluating\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:  response = openai_client.chat.completions.create(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },          { \"role\": \"user\", \"content\": inputs[\"question\"] },      ],  )  return { \"response\": response.choices[0].message.content.strip() }// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: string): Promise<{ response: string }> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response: response.choices[0].message.content?.trim() || \"\" };}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='6. Define evaluator\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# Define instructions for the LLM judge evaluatorinstructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.\"\"\"# Define output schema for the LLM judgeclass Grade(BaseModel):  score: bool = Field(      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"  )# Define LLM judge that grades the accuracy of the response relative to reference outputdef accuracy(outputs: dict, reference_outputs: dict) -> bool:  response = openai_client.beta.chat.completions.parse(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='{ \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer: {outputs[\"response\"]}\"\"\"          },      ],      response_format=Grade,  )  return response.choices[0].message.parsed.score// Define instructions for the LLM judge evaluatorconst instructions = `Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.`;// Define context for the LLM judge evaluatorconst context = `Ground Truth answer: {reference}; Student\\'s Answer: {prediction}`;// Define output schema for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM judge that grades the accuracy of the response relative to reference outputasync function accuracy({outputs,referenceOutputs,}: {outputs?: Record<string, string>;referenceOutputs?: Record<string, string>;}): Promise<EvaluationResult> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: instructions },    { role: \"user\", content: context.replace(\"{prediction}\", outputs?.answer || \"\").replace(\"{reference}\", referenceOutputs?.answer || \"\") }  ],  response_format: zodResponseFormat(ResponseSchema, \"response\")});return {  key: \"accuracy\",  score: ResponseSchema.parse(JSON.parse(response.choices[0].message.content || \"\")).score,};}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='7. Run and view results\\u200b\\nPythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(  target,  data=\"Sample dataset\",  evaluators=[      accuracy,      # can add multiple evaluators here  ],  experiment_prefix=\"first-eval-in-langsmith\",  max_concurrency=2,)// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate((exampleInput) => {  return target(exampleInput.question);},{  data: \"Sample dataset\",  evaluators: [    accuracy,    // can add multiple evaluators here  ],  experimentPrefix: \"first-eval-in-langsmith\",  maxConcurrency: 2,});\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"Next steps\\u200b\\nFor conceptual explanations see the Conceptual guide.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment3. Import dependencies4. Create a dataset5. Define what you're evaluating6. Define evaluator7. Run and view resultsNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbedding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
