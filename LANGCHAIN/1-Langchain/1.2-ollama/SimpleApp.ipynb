{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Gen AI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BASE_MODEL_REPO_ID'] = \"llama3.2:3b\"\n",
    "os.environ['EMBEDDING_MODEL_REPO_ID'] = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_llm = ChatOllama(model=os.environ[\"BASE_MODEL_REPO_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--from the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1fb89c49d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/evaluation\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate an agentHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorConceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationOn this pageEvaluation quick start\\nThis quick start will get you up and running with our evaluation SDK and Experiments UI.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openai pydanticyarn add langsmith openai zod\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=\"<your-langchain-api-key>\"# The example uses OpenAI, but it\\'s not necessary in generalexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n3. Import dependencies\\u200b\\nPythonTypeScriptfrom langsmith import wrappers, Clientfrom pydantic import BaseModel, Fieldfrom openai import OpenAIclient = Client()openai_client = wrappers.wrap_openai(OpenAI())import { Client } from \"langsmith\";import OpenAI from \"openai\";import { z } from \"zod\";import { zodResponseFormat } from \"openai/helpers/zod\";import type { EvaluationResult } from \"langsmith/evaluation\";import { evaluate } from \"langsmith/evaluation\";   const client = new Client();const openai = new OpenAI();\\n4. Create a dataset\\u200b\\nPythonTypeScript# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application# Create inputs and reference outputsexamples = [  (      \"Which country is Mount Kilimanjaro located in?\",      \"Mount Kilimanjaro is located in Tanzania.\",  ),  (      \"What is Earth\\'s lowest point?\",      \"Earth\\'s lowest point is The Dead Sea.\",  ),]inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]outputs = [{\"answer\": output_answer} for _, output_answer in examples]# Programmatically create a dataset in LangSmithdataset = client.create_dataset(  dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application  // Create inputs and reference outputsconst examples: [string, string][] = [[  \"Which country is Mount Kilimanjaro located in?\",  \"Mount Kilimanjaro is located in Tanzania.\",],[  \"What is Earth\\'s lowest point?\",  \"Earth\\'s lowest point is The Dead Sea.\",],];const inputs = examples.map(([inputPrompt]) => ({question: inputPrompt,}));const outputs = examples.map(([, outputAnswer]) => ({answer: outputAnswer,}));// Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait client.createExamples({inputs,outputs,datasetId: dataset.id,});\\n5. Define what you\\'re evaluating\\u200b\\nPythonTypeScript# Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:  response = openai_client.chat.completions.create(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },          { \"role\": \"user\", \"content\": inputs[\"question\"] },      ],  )  return { \"response\": response.choices[0].message.content.strip() }// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: string): Promise<{ response: string }> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response: response.choices[0].message.content?.trim() || \"\" };}\\n6. Define evaluator\\u200b\\nPythonTypeScript# Define instructions for the LLM judge evaluatorinstructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.\"\"\"# Define output schema for the LLM judgeclass Grade(BaseModel):  score: bool = Field(      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"  )# Define LLM judge that grades the accuracy of the response relative to reference outputdef accuracy(outputs: dict, reference_outputs: dict) -> bool:  response = openai_client.beta.chat.completions.parse(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer: {outputs[\"response\"]}\"\"\"          },      ],      response_format=Grade,  )  return response.choices[0].message.parsed.score// Define instructions for the LLM judge evaluatorconst instructions = `Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.`;// Define context for the LLM judge evaluatorconst context = `Ground Truth answer: {reference}; Student\\'s Answer: {prediction}`;// Define output schema for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM judge that grades the accuracy of the response relative to reference outputasync function accuracy({outputs,referenceOutputs,}: {outputs?: Record<string, string>;referenceOutputs?: Record<string, string>;}): Promise<EvaluationResult> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: instructions },    { role: \"user\", content: context.replace(\"{prediction}\", outputs?.answer || \"\").replace(\"{reference}\", referenceOutputs?.answer || \"\") }  ],  response_format: zodResponseFormat(ResponseSchema, \"response\")});return {  key: \"accuracy\",  score: ResponseSchema.parse(JSON.parse(response.choices[0].message.content || \"\")).score,};}\\n7. Run and view results\\u200b\\nPythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(  target,  data=\"Sample dataset\",  evaluators=[      accuracy,      # can add multiple evaluators here  ],  experiment_prefix=\"first-eval-in-langsmith\",  max_concurrency=2,)// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate((exampleInput) => {  return target(exampleInput.question);},{  data: \"Sample dataset\",  evaluators: [    accuracy,    // can add multiple evaluators here  ],  experimentPrefix: \"first-eval-in-langsmith\",  maxConcurrency: 2,});\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.\\n\\nNext steps\\u200b\\nFor conceptual explanations see the Conceptual guide.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment3. Import dependencies4. Create a dataset5. Define what you\\'re evaluating6. Define evaluator7. Run and view resultsNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data--> Docs --> Divide our text into chunks --> vectors --> Vector Embedding --> Vectore store db\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter =RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate an agentHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorConceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationOn this pageEvaluation quick start'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='This quick start will get you up and running with our evaluation SDK and Experiments UI.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openai pydanticyarn add langsmith openai zod\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=\"<your-langchain-api-key>\"# The example uses OpenAI, but it\\'s not necessary in generalexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n3. Import dependencies\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='3. Import dependencies\\u200b\\nPythonTypeScriptfrom langsmith import wrappers, Clientfrom pydantic import BaseModel, Fieldfrom openai import OpenAIclient = Client()openai_client = wrappers.wrap_openai(OpenAI())import { Client } from \"langsmith\";import OpenAI from \"openai\";import { z } from \"zod\";import { zodResponseFormat } from \"openai/helpers/zod\";import type { EvaluationResult } from \"langsmith/evaluation\";import { evaluate } from \"langsmith/evaluation\";   const client = new Client();const openai = new OpenAI();\\n4. Create a dataset\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application# Create inputs and reference outputsexamples = [  (      \"Which country is Mount Kilimanjaro located in?\",      \"Mount Kilimanjaro is located in Tanzania.\",  ),  (      \"What is Earth\\'s lowest point?\",      \"Earth\\'s lowest point is The Dead Sea.\",  ),]inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]outputs = [{\"answer\": output_answer} for _, output_answer in examples]# Programmatically create a dataset in LangSmithdataset = client.create_dataset(  dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='description=\"A sample dataset in LangSmith.\")# Add examples to the datasetclient.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)// For other dataset creation methods, see: // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application  // Create inputs and reference outputsconst examples: [string, string][] = [[  \"Which country is Mount Kilimanjaro located in?\",  \"Mount Kilimanjaro is located in Tanzania.\",],[  \"What is Earth\\'s lowest point?\",  \"Earth\\'s lowest point is The Dead Sea.\",],];const inputs = examples.map(([inputPrompt]) => ({question: inputPrompt,}));const outputs = examples.map(([, outputAnswer]) => ({answer: outputAnswer,}));// Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='Programmatically create a dataset in LangSmithconst dataset = await client.createDataset(\"Sample dataset\", {description: \"A sample dataset in LangSmith.\",});// Add examples to the datasetawait client.createExamples({inputs,outputs,datasetId: dataset.id,});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"5. Define what you're evaluating\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:  response = openai_client.chat.completions.create(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },          { \"role\": \"user\", \"content\": inputs[\"question\"] },      ],  )  return { \"response\": response.choices[0].message.content.strip() }// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: string): Promise<{ response: string }> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: \"Answer the following question accurately\" },    { role: \"user\", content: inputs },  ],});return { response: response.choices[0].message.content?.trim() || \"\" };}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='6. Define evaluator\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='PythonTypeScript# Define instructions for the LLM judge evaluatorinstructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.\"\"\"# Define output schema for the LLM judgeclass Grade(BaseModel):  score: bool = Field(      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"  )# Define LLM judge that grades the accuracy of the response relative to reference outputdef accuracy(outputs: dict, reference_outputs: dict) -> bool:  response = openai_client.beta.chat.completions.parse(      model=\"gpt-4o-mini\",      messages=[          { \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='{ \"role\": \"system\", \"content\": instructions },          {              \"role\": \"user\",              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};               Student\\'s Answer: {outputs[\"response\"]}\"\"\"          },      ],      response_format=Grade,  )  return response.choices[0].message.parsed.score// Define instructions for the LLM judge evaluatorconst instructions = `Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.`;// Define context for the LLM judge evaluatorconst context = `Ground Truth answer: {reference}; Student\\'s Answer: {prediction}`;// Define output schema for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='for the LLM judgeconst ResponseSchema = z.object({score: z  .boolean()  .describe(    \"Boolean that indicates whether the response is accurate relative to the reference answer\"  ),});// Define LLM judge that grades the accuracy of the response relative to reference outputasync function accuracy({outputs,referenceOutputs,}: {outputs?: Record<string, string>;referenceOutputs?: Record<string, string>;}): Promise<EvaluationResult> {const response = await openai.chat.completions.create({  model: \"gpt-4o-mini\",  messages: [    { role: \"system\", content: instructions },    { role: \"user\", content: context.replace(\"{prediction}\", outputs?.answer || \"\").replace(\"{reference}\", referenceOutputs?.answer || \"\") }  ],  response_format: zodResponseFormat(ResponseSchema, \"response\")});return {  key: \"accuracy\",  score: ResponseSchema.parse(JSON.parse(response.choices[0].message.content || \"\")).score,};}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content='7. Run and view results\\u200b\\nPythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(  target,  data=\"Sample dataset\",  evaluators=[      accuracy,      # can add multiple evaluators here  ],  experiment_prefix=\"first-eval-in-langsmith\",  max_concurrency=2,)// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate((exampleInput) => {  return target(exampleInput.question);},{  data: \"Sample dataset\",  evaluators: [    accuracy,    // can add multiple evaluators here  ],  experimentPrefix: \"first-eval-in-langsmith\",  maxConcurrency: 2,});\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation quick start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This quick start will get you up and running with our evaluation SDK and Experiments UI.', 'language': 'en'}, page_content=\"Next steps\\u200b\\nFor conceptual explanations see the Conceptual guide.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment3. Import dependencies4. Create a dataset5. Define what you're evaluating6. Define evaluator7. Run and view resultsNext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "# model_kwargs = {\"device\": \"cuda\"}\n",
    "# encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceEmbedding(model_name=model_name)\n",
    "Settings.embed_model = hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Document\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:260\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    252\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    253\u001b[0m     EmbeddingStartEvent(\n\u001b[0;32m    254\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m )\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    258\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[0;32m    259\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 260\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    263\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    264\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text],\n\u001b[0;32m    265\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [text_embedding],\n\u001b[0;32m    266\u001b[0m         }\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    268\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    269\u001b[0m     EmbeddingEndEvent(\n\u001b[0;32m    270\u001b[0m         chunks\u001b[38;5;241m=\u001b[39m[text],\n\u001b[0;32m    271\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[text_embedding],\n\u001b[0;32m    272\u001b[0m     )\n\u001b[0;32m    273\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\llama_index\\embeddings\\huggingface\\base.py:266\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._get_text_embedding\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates Embeddings for text.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m        List[float]: numpy array of embeddings\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\llama_index\\embeddings\\huggingface\\base.py:215\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._embed\u001b[1;34m(self, sentences, prompt_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool\u001b[38;5;241m=\u001b[39mpool)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emb\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:572\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m extra_features \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 572\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# Some models (e.g. INSTRUCTOR, GRIT) require removing the prompt before pooling\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Tracking the prompt length allow us to remove the prompt during pooling\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     tokenized_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize([prompt])\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"Document\") to str"
     ]
    }
   ],
   "source": [
    "embeddings = hf.get_text_embedding(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbedding' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 3\u001b[0m vectorstoredb \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\vectorstores\\base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\vectorstores\\faiss.py:1041\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1043\u001b[0m         texts,\n\u001b[0;32m   1044\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1049\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HuggingFaceEmbedding' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
